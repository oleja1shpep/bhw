# ЧЕКПОИНТ

В качестве оптимизатора я выбрал SGD

Сначала я взял тупо блок из 1 домашки и он дал качество что-то около 20% на валидации на 15 эпохах

Затем я добавил в сеть второй блок. Получил следующее:

**My Net**
* SGD(lr=0.1, momentum=0.9)
* 15 эпох

Train: 44.8%
Val: 23.5%
Test: 20.7%


# Дальнейшие эксперименты

Затем я решил добавить аугментацию RandomHorizontalFlip и начал эксперименты с ResNet

**ResNet18**
* SGD(lr=0.01, momentum=0.9)
* 20 эпох

Train: -
Val: -
Test: 28%

Прочерки потому что не помню результатов

Затем я посмотрел на архитектуру ResNet18 и увидел что там в самом начале используется MaxPooling. Для больших картинок он очевидно нужен, но для маленьких - только мешает. Я его убрал

Помимо этого там есть свёртки с stride = 2. Для маленьких картинок как у нас - это не есть хорошо. Я сделал stride = 1 в первых трёх свёртках, где stride равнялся 2

Помимо этого я решил поиграться с SGD, накинув туда ещё параметров

**My ResNet18**
* SGD(lr=0.01, momentum=0.9, weight_decay=1e-4, nesterov=True)
* 10 эпох

Train: 44%
Val: 37%
Test: 34.8%

Потом решил поэксперементировать с efficient net. Взял efficientnet_v2_s на 10 эпох и сделал stride = 1 в первых двух свёрточных слоях со stride = 2

Помимо этого я добавил scheduler CosineAnnealingLR(T_max = 15). Сделал это потому, что заметил что на поздних эпохах возникает переобучение, попытался так с ним бороться, плавно уменьшая lr

**My efficientnet_v2_s**
* SGD(lr=0.01, momentum=0.9, weight_decay=1e-4, nesterov=True)
* CosineAnnealingLR(T_max = 15)
* 15 эпох

Train: 51.9%
Val: 39.8%
Test: 37.56%